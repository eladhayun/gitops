apiVersion: batch/v1
kind: Job
metadata:
  name: email-import
  namespace: ids
  labels:
    app: email-import
    job-type: data-import
spec:
  # Set backoffLimit to avoid infinite retries on failure
  backoffLimit: 3
  # Don't auto-delete - cleanup CronJob will keep only the latest completed job
  # ttlSecondsAfterFinished: 86400
  template:
    metadata:
      labels:
        app: email-import
        job-type: data-import
    spec:
      restartPolicy: Never
      serviceAccountName: email-import-sa
      
      # Init container to download files from Azure Blob Storage
      initContainers:
      - name: download-emails
        image: mcr.microsoft.com/azure-cli:latest
        command:
          - /bin/sh
          - -c
          - |
            set -e
            echo "=========================================="
            echo "   EMAIL DOWNLOAD FROM AZURE BLOB"
            echo "=========================================="
            echo "Storage Account: ${AZURE_STORAGE_ACCOUNT}"
            echo "Container: ${AZURE_CONTAINER_NAME}"
            echo "Start Time: $(date '+%Y-%m-%d %H:%M:%S')"
            echo ""
            
            # Create download directory
            mkdir -p /emails
            
            # Record start time
            START_TIME=$(date +%s)
            
            # List all files to download first
            echo "üìã Listing files to download..."
            FILE_LIST=$(az storage blob list \
              --account-name ${AZURE_STORAGE_ACCOUNT} \
              --account-key ${AZURE_STORAGE_KEY} \
              --container-name ${AZURE_CONTAINER_NAME} \
              --query "[].{name:name, size:properties.contentLength}" \
              --output json)
            
            TOTAL_FILES=$(echo "$FILE_LIST" | jq '. | length')
            TOTAL_BYTES=$(echo "$FILE_LIST" | jq '[.[].size] | add // 0')
            TOTAL_SIZE_MB=$(echo "scale=2; $TOTAL_BYTES / 1024 / 1024" | bc)
            TOTAL_SIZE_GB=$(echo "scale=2; $TOTAL_BYTES / 1024 / 1024 / 1024" | bc)
            
            echo "Found $TOTAL_FILES file(s) to download"
            if [ $(echo "$TOTAL_SIZE_GB > 1" | bc) -eq 1 ]; then
              echo "Total size: ${TOTAL_SIZE_GB} GB"
            else
              echo "Total size: ${TOTAL_SIZE_MB} MB"
            fi
            echo ""
            
            # Download files one by one with progress
            CURRENT=0
            echo "$FILE_LIST" | jq -r '.[] | "\(.name)|\(.size)"' | while IFS='|' read -r filename filesize; do
              CURRENT=$((CURRENT + 1))
              SIZE_MB=$(echo "scale=2; $filesize / 1024 / 1024" | bc)
              
              echo "----------------------------------------"
              echo "üì• [$CURRENT/$TOTAL_FILES] Downloading: $filename"
              echo "   Size: ${SIZE_MB} MB"
              echo "   Progress: $(echo "scale=1; $CURRENT * 100 / $TOTAL_FILES" | bc)%"
              
              FILE_START=$(date +%s)
              
              az storage blob download \
                --account-name ${AZURE_STORAGE_ACCOUNT} \
                --account-key ${AZURE_STORAGE_KEY} \
                --container-name ${AZURE_CONTAINER_NAME} \
                --name "$filename" \
                --file "/emails/$filename" \
                --no-progress \
                --output none 2>&1
              
              FILE_END=$(date +%s)
              FILE_DURATION=$((FILE_END - FILE_START))
              
              if [ -f "/emails/$filename" ]; then
                if [ $FILE_DURATION -gt 0 ]; then
                  RATE=$(echo "scale=2; $SIZE_MB / $FILE_DURATION" | bc)
                  echo "   ‚úÖ Downloaded in ${FILE_DURATION}s (${RATE} MB/s)"
                else
                  echo "   ‚úÖ Downloaded"
                fi
              else
                echo "   ‚ùå Failed"
              fi
            done
            
            # Calculate download time and stats
            END_TIME=$(date +%s)
            DURATION=$((END_TIME - START_TIME))
            MINUTES=$((DURATION / 60))
            SECONDS=$((DURATION % 60))
            
            DOWNLOADED_SIZE=$(du -sh /emails 2>/dev/null | cut -f1 || echo "0")
            DOWNLOADED_COUNT=$(find /emails -type f 2>/dev/null | wc -l | tr -d ' ')
            
            echo ""
            echo "=========================================="
            echo "   DOWNLOAD COMPLETE"
            echo "=========================================="
            echo "End Time: $(date '+%Y-%m-%d %H:%M:%S')"
            echo "Duration: ${MINUTES}m ${SECONDS}s"
            echo "Downloaded: ${DOWNLOADED_COUNT} file(s)"
            echo "Total Size: ${DOWNLOADED_SIZE}"
            if [ $DURATION -gt 0 ]; then
              AVG_RATE=$(echo "scale=2; $TOTAL_SIZE_MB / $DURATION" | bc)
              echo "Avg Speed: ${AVG_RATE} MB/s"
            fi
            echo ""
            echo "üìÅ Downloaded files:"
            ls -lh /emails | tail -n +2 | awk '{printf "   %s  %-50s  %s\n", $5, $9, ""}'
            echo "=========================================="
        env:
        - name: AZURE_STORAGE_ACCOUNT
          valueFrom:
            secretKeyRef:
              name: azure-storage-secret
              key: storage-account-name
        - name: AZURE_STORAGE_KEY
          valueFrom:
            secretKeyRef:
              name: azure-storage-secret
              key: storage-account-key
        - name: AZURE_CONTAINER_NAME
          value: "email-imports"
        volumeMounts:
        - name: email-data
          mountPath: /emails
      
      # Main container to import emails into database
      containers:
      - name: import-emails
        image: prodacr1234.azurecr.io/ids-backend:latest
        command:
          - /bin/sh
          - -c
          - |
            set -e
            echo "===== Starting Email Import Process ====="
            
            # Count files
            eml_count=$(find /emails -name "*.eml" -type f | wc -l)
            mbox_count=$(find /emails -name "*.mbox" -type f | wc -l)
            
            echo "Found $eml_count EML files and $mbox_count MBOX files"
            
            # Import EML files
            if [ "$eml_count" -gt 0 ]; then
              echo "===== Importing EML files ====="
              /app/bin/import-emails -eml /emails -embeddings=true
            fi
            
            # Import MBOX files (one by one if multiple exist)
            if [ "$mbox_count" -gt 0 ]; then
              echo "===== Importing MBOX files ====="
              find /emails -name "*.mbox" -type f | while read mbox_file; do
                echo "Processing: $mbox_file"
                /app/bin/import-emails -mbox "$mbox_file" -embeddings=true
              done
            fi
            
            echo "===== Email Import Complete ====="
            echo "Summary:"
            echo "  - EML files processed: $eml_count"
            echo "  - MBOX files processed: $mbox_count"
        env:
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: ids-secrets
              key: database-url
        - name: OPENAI_API_KEY
          valueFrom:
            secretKeyRef:
              name: ids-secrets
              key: openai-api-key
        - name: WAIT_FOR_TUNNEL
          value: "false"
        volumeMounts:
        - name: email-data
          mountPath: /emails
        resources:
          requests:
            memory: "1Gi"      # Increased for large file processing
            cpu: "500m"
          limits:
            memory: "4Gi"      # Allows processing very large MBOX files
            cpu: "2000m"
      
      volumes:
      - name: email-data
        emptyDir:
          sizeLimit: 75Gi  # Increased for 70GB+ email imports (uses FREE node ephemeral storage)

