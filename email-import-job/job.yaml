apiVersion: batch/v1
kind: Job
metadata:
  name: email-import
  namespace: ids
  labels:
    app: email-import
    job-type: data-import
spec:
  # Set backoffLimit to avoid infinite retries on failure
  backoffLimit: 3
  # Don't auto-delete - cleanup CronJob will keep only the latest completed job
  # ttlSecondsAfterFinished: 86400
  template:
    metadata:
      labels:
        app: email-import
        job-type: data-import
    spec:
      restartPolicy: Never
      serviceAccountName: email-import-sa
      
      # Init container to download files from Azure Blob Storage
      initContainers:
      - name: download-emails
        image: mcr.microsoft.com/azure-cli:latest
        command:
          - /bin/sh
          - -c
          - |
            set -e
            echo ""
            echo "==========================================="
            echo "  EMAIL DOWNLOAD FROM AZURE BLOB STORAGE"
            echo "==========================================="
            echo "Storage: ${AZURE_STORAGE_ACCOUNT}"
            echo "Container: ${AZURE_CONTAINER_NAME}"
            echo "Started: $(date '+%Y-%m-%d %H:%M:%S')"
            echo ""
            
            # Create download directory
            mkdir -p /emails
            START_TIME=$(date +%s)
            
            # Check if files already exist (from previous runs)
            EXISTING_SIZE=$(du -hs /emails 2>/dev/null | cut -f1 || echo "0")
            FILE_COUNT=$(find /emails -type f 2>/dev/null | wc -l | tr -d ' ')
            
            if [ "$FILE_COUNT" -gt 0 ]; then
              echo "===== Files Already Downloaded ====="
              echo "Found $FILE_COUNT existing files ($EXISTING_SIZE)"
              echo "Skipping download - using cached files from previous run"
              echo "===== Download Skipped ====="
            else
              echo "No existing files found - downloading from Azure..."
              az storage blob download-batch \
                --account-name ${AZURE_STORAGE_ACCOUNT} \
                --account-key ${AZURE_STORAGE_KEY} \
                --source ${AZURE_CONTAINER_NAME} \
                --destination /emails \
                --pattern "*" \
                --output table
              
              END_TIME=$(date +%s)
              DURATION=$((END_TIME - START_TIME))
              MINUTES=$((DURATION / 60))
              SECONDS=$((DURATION % 60))
              
              echo ""
              echo "==========================================="
              echo "  DOWNLOAD COMPLETE"
              echo "==========================================="
              echo "Finished: $(date '+%Y-%m-%d %H:%M:%S')"
              echo "Duration: ${MINUTES}m ${SECONDS}s"
              
              FINAL_SIZE=$(du -hs /emails 2>/dev/null | cut -f1 || echo "unknown")
              FILE_COUNT=$(find /emails -type f 2>/dev/null | wc -l | tr -d ' ')
              
              echo "Total Size: $FINAL_SIZE"
              echo "File Count: $FILE_COUNT"
              echo "==========================================="
            fi
        env:
        - name: AZURE_STORAGE_ACCOUNT
          valueFrom:
            secretKeyRef:
              name: azure-storage-secret
              key: storage-account-name
        - name: AZURE_STORAGE_KEY
          valueFrom:
            secretKeyRef:
              name: azure-storage-secret
              key: storage-account-key
        - name: AZURE_CONTAINER_NAME
          value: "email-imports"
        volumeMounts:
        - name: email-data
          mountPath: /emails
      
      # Main container - just verifies download completed successfully
      containers:
      - name: verify-download
        image: alpine:latest
        command:
          - /bin/sh
          - -c
          - |
            set -e
            echo "===== Verifying Downloaded Emails ====="
            
            # Count files
            eml_count=$(find /emails -name "*.eml" -type f 2>/dev/null | wc -l | tr -d ' ')
            mbox_count=$(find /emails -name "*.mbox" -type f 2>/dev/null | wc -l | tr -d ' ')
            total_size=$(du -hs /emails 2>/dev/null | cut -f1 || echo "0")
            
            echo "Download verified:"
            echo "  - EML files: $eml_count"
            echo "  - MBOX files: $mbox_count"
            echo "  - Total size: $total_size"
            echo ""
            echo "âœ… Files ready for backend processing"
            echo "The IDS backend will automatically import these emails."
        volumeMounts:
        - name: email-data
          mountPath: /emails
        resources:
          requests:
            memory: "64Mi"
            cpu: "50m"
          limits:
            memory: "128Mi"
            cpu: "100m"
      
      volumes:
      - name: email-data
        persistentVolumeClaim:
          claimName: email-data-pvc  # Persistent storage - files survive between job runs!

