apiVersion: batch/v1
kind: Job
metadata:
  name: email-import
  namespace: ids
  labels:
    app: email-import
    job-type: data-import
spec:
  # Set backoffLimit to avoid infinite retries on failure
  backoffLimit: 3
  # Don't auto-delete - cleanup CronJob will keep only the latest completed job
  # ttlSecondsAfterFinished: 86400
  template:
    metadata:
      labels:
        app: email-import
        job-type: data-import
    spec:
      restartPolicy: Never
      serviceAccountName: email-import-sa
      
      # Init container to download files from Azure Blob Storage
      initContainers:
      - name: download-emails
        image: mcr.microsoft.com/azure-cli:latest
        command:
          - /bin/sh
          - -c
          - |
            set -e
            echo ""
            echo "==========================================="
            echo "  EMAIL DOWNLOAD FROM AZURE BLOB STORAGE"
            echo "==========================================="
            echo "Storage: ${AZURE_STORAGE_ACCOUNT}"
            echo "Container: ${AZURE_CONTAINER_NAME}"
            echo "Started: $(date '+%Y-%m-%d %H:%M:%S')"
            echo ""
            
            # Create download directory
            mkdir -p /emails
            START_TIME=$(date +%s)
            
            # Check if files already exist (from previous runs)
            EXISTING_SIZE=$(du -hs /emails 2>/dev/null | cut -f1 || echo "0")
            FILE_COUNT=$(find /emails -type f 2>/dev/null | wc -l | tr -d ' ')
            
            if [ "$FILE_COUNT" -gt 0 ]; then
              echo "===== Files Already Downloaded ====="
              echo "Found $FILE_COUNT existing files ($EXISTING_SIZE)"
              echo "Skipping download - using cached files from previous run"
              echo "===== Download Skipped ====="
            else
              echo "No existing files found - downloading from Azure..."
              az storage blob download-batch \
                --account-name ${AZURE_STORAGE_ACCOUNT} \
                --account-key ${AZURE_STORAGE_KEY} \
                --source ${AZURE_CONTAINER_NAME} \
                --destination /emails \
                --pattern "*" \
                --output table
              
              END_TIME=$(date +%s)
              DURATION=$((END_TIME - START_TIME))
              MINUTES=$((DURATION / 60))
              SECONDS=$((DURATION % 60))
              
              echo ""
              echo "==========================================="
              echo "  DOWNLOAD COMPLETE"
              echo "==========================================="
              echo "Finished: $(date '+%Y-%m-%d %H:%M:%S')"
              echo "Duration: ${MINUTES}m ${SECONDS}s"
              
              FINAL_SIZE=$(du -hs /emails 2>/dev/null | cut -f1 || echo "unknown")
              FILE_COUNT=$(find /emails -type f 2>/dev/null | wc -l | tr -d ' ')
              
              echo "Total Size: $FINAL_SIZE"
              echo "File Count: $FILE_COUNT"
              echo "==========================================="
            fi
        env:
        - name: AZURE_STORAGE_ACCOUNT
          valueFrom:
            secretKeyRef:
              name: azure-storage-secret
              key: storage-account-name
        - name: AZURE_STORAGE_KEY
          valueFrom:
            secretKeyRef:
              name: azure-storage-secret
              key: storage-account-key
        - name: AZURE_CONTAINER_NAME
          value: "email-imports"
        volumeMounts:
        - name: email-data
          mountPath: /emails
      
      # Main container - processes emails end-to-end
      containers:
      - name: process-emails
        image: prodacr1234.azurecr.io/ids:latest  # Will be replaced by job trigger
        command:
          - /bin/sh
          - -c
          - |
            set -e
            echo ""
            echo "==========================================="
            echo "  EMAIL PROCESSING - END TO END"
            echo "==========================================="
            echo "Started: $(date '+%Y-%m-%d %H:%M:%S')"
            echo ""
            
            # Count downloaded files
            eml_count=$(find /emails -name "*.eml" -type f 2>/dev/null | wc -l | tr -d ' ')
            mbox_count=$(find /emails -name "*.mbox" -type f 2>/dev/null | wc -l | tr -d ' ')
            total_size=$(du -hs /emails 2>/dev/null | cut -f1 || echo "0")
            
            echo "===== Downloaded Files ====="
            echo "  - EML files: $eml_count"
            echo "  - MBOX files: $mbox_count"
            echo "  - Total size: $total_size"
            echo ""
            
            # Run email import
            echo "===== Step 1/3: Importing Emails to Database ====="
            /home/appuser/import-emails --eml-path /emails --mbox-path /emails
            
            echo ""
            echo "===== Step 2/3: Generating Email Embeddings ====="
            # Email embeddings are generated during import
            echo "✓ Email embeddings generated"
            
            echo ""
            echo "===== Step 3/3: Generating Thread Embeddings ====="
            # Thread embeddings are generated during import
            echo "✓ Thread embeddings generated"
            
            echo ""
            echo "==========================================="
            echo "  PROCESSING COMPLETE"
            echo "==========================================="
            echo "Finished: $(date '+%Y-%m-%d %H:%M:%S')"
            echo "✅ All emails imported and embedded successfully"
            echo "==========================================="
        env:
        - name: EMBEDDINGS_DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: ids-postgres-secret
              key: DATABASE_URL
        - name: OPENAI_API_KEY
          valueFrom:
            secretKeyRef:
              name: ids-secrets
              key: OPENAI_API_KEY
        volumeMounts:
        - name: email-data
          mountPath: /emails
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "4Gi"
            cpu: "2000m"
      
      volumes:
      - name: email-data
        emptyDir: {}  # Temporary storage - automatically cleaned up after job completes

